{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "spark = SparkSession.builder.appName(\"ALS\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+----------+--------+------+--------------------+------+---------------+--------------------+--------+----------+----------------+--------+--------------------+------+----------+------------+---------+--------------------------+---------------+--------------------+----------+-----------+--------------------+------------+-------------+------+----+\n",
      "|cons_id|prefix|firstname|middlename|lastname|suffix|          salutation|gender|       birth_dt|               title|employer|occupation|          income|  source|           subsource|userid|  password|is_validated|is_banned|change_password_next_login|consent_type_id|           create_dt|create_app|create_user|         modified_dt|modified_app|modified_user|status|note|\n",
      "+-------+------+---------+----------+--------+------+--------------------+------+---------------+--------------------+--------+----------+----------------+--------+--------------------+------+----------+------------+---------+--------------------------+---------------+--------------------+----------+-----------+--------------------+------------+-------------+------+----+\n",
      "|      1|  null|     null|       Lee|    null|    MD|                null|     E|           null|vSkSIzEQJdXnqeTTTXSG|    null|      null|29716063420.7735|  google|                null|  3663|_kXcXaoK7i|           1|        0|                         0|           5958|Fri, 1983-08-26 0...|      1484|       6162|Sun, 2015-12-27 0...|        4022|         6349|     1|null|\n",
      "|      2|  null|     null|      null|    null|    II|boFqBKgLlSgEZsFrgCZd|     E|Mon, 2004-11-15|                null|    null|      null|        671.7468|facebook|pRzBAZSGNScwCyreCEYr|  7125|Ll3ZUxnh*9|           0|        0|                         1|           4236|Mon, 1979-03-05 2...|      4176|       5476|Tue, 1989-06-20 1...|        9010|         5698|     1|null|\n",
      "+-------+------+---------+----------+--------+------+--------------------+------+---------------+--------------------+--------+----------+----------------+--------+--------------------+------+----------+------------+---------+--------------------------+---------------+--------------------+----------+-----------+--------------------+------------+-------------+------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkFiles\n",
    "# read cons.csv file into spark DataFrame\n",
    "url = \"https://storage.googleapis.com/sharp-matter-286015.appspot.com/cons.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df_cons = spark.read.csv(SparkFiles.get(\"cons.csv\"), sep=\",\", header=True)\n",
    "#df_cons = spark.read.csv(\"../put csv here/cons.csv\",header=True)\n",
    "#df = spark.read.format(\"csv\").load(\"cons.csv\")\n",
    "\n",
    "# # select required columns and alias as per requirement\n",
    "df_cons = df_cons.select(col(\"cons_id\"), col(\"create_dt\"),col(\"modified_dt\").alias(\"updated_dt\")\\\n",
    "                          , col(\"subsource\").alias(\"code\"))\n",
    "\n",
    "# # Show DataFrame\n",
    "df_con.show(n=2)\n",
    "type(df_cons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+--------------------+------+\n",
      "|cons_id|cons_email_id|               email|status|\n",
      "+-------+-------------+--------------------+------+\n",
      "| 548198|            1|xmartinez@vincent...|     1|\n",
      "| 491137|            2|  hmiller@haynes.biz|     1|\n",
      "+-------+-------------+--------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read cons_email.csv file into spark DataFrame\n",
    "url = \"https://storage.googleapis.com/sharp-matter-286015.appspot.com/cons_email.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df_cons_email = spark.read.csv(SparkFiles.get(\"cons_email.csv\"), sep=\",\", header=True)\n",
    "\n",
    "#df_cons_email = spark.read.csv(\"../put csv here/cons_email.csv\",header=True)\n",
    "# select required columns and alias as per requirement\n",
    "df_cons_email = df_cons_email.select(col(\"cons_id\"), col(\"cons_email_id\"),col(\"email\"), col(\"status\"))\n",
    "df_cons_email.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-------------+----------+-------+--------------------+--------------------+\n",
      "|cons_email_chapter_subscription_id|cons_email_id|chapter_id|isunsub|            unsub_dt|         modified_dt|\n",
      "+----------------------------------+-------------+----------+-------+--------------------+--------------------+\n",
      "|                                 1|       332188|         1|      1|Sat, 1971-06-12 1...|Thu, 1990-06-28 1...|\n",
      "|                                 2|       536526|         1|      1|Wed, 2006-07-12 0...|Thu, 1979-09-20 0...|\n",
      "+----------------------------------+-------------+----------+-------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read cons_email.csv file into spark DataFrame\n",
    "url = \"https://storage.googleapis.com/sharp-matter-286015.appspot.com/cons_email_chapter_subscription.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df_cons_email_chapter_subscription_email = spark.read.csv(SparkFiles.get(\"cons_email_chapter_subscription.csv\"), sep=\",\", header=True)\n",
    "#df_cons_email_chapter_subscription_email = spark.read.csv(\"../put csv here/cons_email_chapter_subscription.csv\",header=True)\n",
    "df_cons_email_chapter_subscription_email=df_cons_email_chapter_subscription_email.\\\n",
    "filter(df_cons_email_chapter_subscription_email.chapter_id==1)\n",
    "df_cons_email_chapter_subscription_email.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+-------------+--------------------+------+\n",
      "|cons_id|           create_dt|          updated_dt|                code|cons_email_id|               email|status|\n",
      "+-------+--------------------+--------------------+--------------------+-------------+--------------------+------+\n",
      "| 100010|Sat, 2019-09-14 1...|Sat, 2013-11-23 2...|                null|       630340|stephanie18@cox.info|     1|\n",
      "| 100140|Thu, 1999-09-23 0...|Mon, 1987-03-02 1...|pUXsobfAnPYGUGbOMEtz|       289643|tracirich@hickman...|     1|\n",
      "+-------+--------------------+--------------------+--------------------+-------------+--------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1400000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_join_cons_id = df_cons.join(df_cons_email, on=['cons_id'], how='inner')\n",
    "df_join_cons_id.show(n=2)\n",
    "df_join_cons_id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+--------------------+--------------------+--------------------+--------------------+------+----------------------------------+----------+-------+--------+-----------+\n",
      "|cons_email_id|cons_id|           create_dt|          updated_dt|                code|               email|status|cons_email_chapter_subscription_id|chapter_id|isunsub|unsub_dt|modified_dt|\n",
      "+-------------+-------+--------------------+--------------------+--------------------+--------------------+------+----------------------------------+----------+-------+--------+-----------+\n",
      "|       100010| 386479|Mon, 1972-01-17 1...|Thu, 2011-06-16 2...|DAQWqTYFyOUiyuMTCLGi| scottcody@yahoo.com|     1|                              null|      null|   null|    null|       null|\n",
      "|      1000240| 188649|Fri, 1979-12-07 0...|Wed, 1985-07-24 2...|                null|collinsjuan@hotma...|     1|                              null|      null|   null|    null|       null|\n",
      "+-------------+-------+--------------------+--------------------+--------------------+--------------------+------+----------------------------------+----------+-------+--------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1400000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_join_cons_email_id = df_join_cons_id.join(df_cons_email_chapter_subscription_email, on=['cons_email_id'], how='inner')\n",
    "df_join_cons_email_id.show(n=2)\n",
    "df_join_cons_email_id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to CSV file \n",
    "df_join_cons_email_id.coalesce(1).write.csv('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split date coulmn and only pic date remove time and day\n",
    "from pyspark.sql.functions import split\n",
    "df_join_cons_email_id_date=df_join_cons_email_id.withColumn(\"create_dt\", split(col(\"create_dt\"), \" \").getItem(1))\\\n",
    ".withColumn(\"updated_dt\", split(col(\"updated_dt\"), \" \").getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert created date and updated date to date time format\n",
    "df_join_cons_email_id_type = df_join_cons_email_id_date.selectExpr(\"email\", \"code\",\\\n",
    "    \"cast(isunsub as boolean) is_unsub\",\\\n",
    "    \"cast(create_dt as date) create_dt\",\"cast(updated_dt as date) updated_dt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- email: string (nullable = true)\n",
      " |-- code: string (nullable = true)\n",
      " |-- is_unsub: boolean (nullable = true)\n",
      " |-- create_dt: date (nullable = true)\n",
      " |-- updated_dt: date (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------+----------+----------+\n",
      "|               email|                code|is_unsub| create_dt|updated_dt|\n",
      "+--------------------+--------------------+--------+----------+----------+\n",
      "|stephanie18@cox.info|                null|    null|2019-09-14|2013-11-23|\n",
      "|tracirich@hickman...|pUXsobfAnPYGUGbOMEtz|    null|1999-09-23|1987-03-02|\n",
      "|hoodjeffrey@gmail...|mayHRqXIkQpNFXCfEEbA|    true|1992-09-26|1971-05-09|\n",
      "|zhubbard@turner-b...|mayHRqXIkQpNFXCfEEbA|    null|1992-09-26|1971-05-09|\n",
      "|cortezamanda@gmai...|ZtNdsqNSaDggTuWOyBDS|    true|2019-09-21|1992-01-19|\n",
      "|meadowsdana@velez...|ZtNdsqNSaDggTuWOyBDS|    null|2019-09-21|1992-01-19|\n",
      "|john60@callahan-c...|ZtNdsqNSaDggTuWOyBDS|    null|2019-09-21|1992-01-19|\n",
      "|  thomas22@yahoo.com|QdwUCfcdJmRPaPFgymhB|    null|1973-03-22|2005-07-24|\n",
      "|whitepatrick@wood...|QdwUCfcdJmRPaPFgymhB|    null|1973-03-22|2005-07-24|\n",
      "|pamela77@taylor.info|aXTrRwcJvnrsAsDKKTLs|    true|1986-04-08|2020-02-29|\n",
      "|torrestammie@hotm...|kFxYAxIFFuDumHTJGutQ|    true|2018-01-12|2007-06-16|\n",
      "| priceanne@yahoo.com|kFxYAxIFFuDumHTJGutQ|    null|2018-01-12|2007-06-16|\n",
      "|michellesmith@lee...|gWPhtyBZnWZhlhxzoqMF|    null|2000-12-27|1987-04-20|\n",
      "|ashleyschultz@gal...|gWPhtyBZnWZhlhxzoqMF|    null|2000-12-27|1987-04-20|\n",
      "|jenkinskelly@hotm...|                null|    true|1977-06-20|2002-11-07|\n",
      "|stephenclark@yaho...|                null|    true|1977-06-20|2002-11-07|\n",
      "|   nmckay@brooks.biz|                null|    null|1977-06-20|2002-11-07|\n",
      "|  willie48@gmail.com|AVUmYSavnHIrjjNBpsPy|    null|2013-12-08|1986-12-08|\n",
      "|    greyes@davis.org|ofbueUYhZFdwKBytSiXC|    null|1974-09-15|2015-09-19|\n",
      "|lisa32@gutierrez.com|wquyobNHZgJScdWzNqFj|    null|1983-12-19|1989-03-19|\n",
      "+--------------------+--------------------+--------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join_cons_email_id_type.printSchema()\n",
    "df_join_cons_email_id_type.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "df_join_cons_email_id_type_group=df_join_cons_email_id_type.groupBy(\"create_dt\")\\\n",
    ".agg(func.count(func.lit(1)).alias(\"Number of constituents\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------+\n",
      "| create_dt|Number of constituents|\n",
      "+----------+----------------------+\n",
      "|2006-05-21|                    97|\n",
      "|1983-03-12|                    82|\n",
      "|1991-03-26|                    78|\n",
      "|2009-11-22|                    80|\n",
      "|2000-07-03|                    99|\n",
      "|2015-05-19|                    71|\n",
      "|1999-08-14|                    93|\n",
      "|2002-06-20|                    63|\n",
      "|1971-04-09|                    69|\n",
      "|1983-07-07|                    78|\n",
      "|1991-06-13|                    67|\n",
      "|1973-05-23|                    79|\n",
      "|1991-11-09|                    90|\n",
      "|1995-12-01|                    53|\n",
      "|1980-05-22|                    56|\n",
      "|1984-07-06|                    79|\n",
      "|2014-09-26|                    59|\n",
      "|1977-11-06|                    76|\n",
      "|2018-05-28|                    84|\n",
      "|1987-07-08|                    77|\n",
      "+----------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join_cons_email_id_type_group.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_cons_email_id_type_group.coalesce(1).write.csv('acquisition_facts')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit ('PythonData': conda)",
   "language": "python",
   "name": "python36764bitpythondatacondaa95fec0fc623410d81b52d9dd08312c5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
